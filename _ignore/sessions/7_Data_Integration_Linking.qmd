---
title: "Data integration and linking (with survey data)"
subtitle: "Advanced Geospatial Data Processing for Social Scientists"
author: "Dennis Abel & Stefan JÃ¼nger"
date: April 29, 2025
execute:
  echo: true
format:
  revealjs:
    embed-resources: true
    theme: [simple, tweaks.css]
    smaller: true
    scrollable: true
    slide-number: "c/t"
    logo: ../img/GESIS-Logo_2024.svg.png
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false
library(dplyr)
library(sf)
library(terra)
library(tmap)
library(keyring)
library(rnaturalearth)
library(tidyverse)
```

```{r}
#| echo: false
source("course_content.R") 

course_content |> 
  kableExtra::row_spec(12, background = "yellow")
```

## Survey data: ISSP Environment (1993-2020)

After downloading the ISSP data file from the website, we placed it in our .`/data` folder. We have already prepared a script in the folder `./R` called `prep_issp.r` to prepare the data, which we call using `source()`. 

```{r}
#| eval: false
source("./R/prep_issp.R")

head(issp, 10)
```

```{r}
#| echo: false
source("prep_issp.R")

head(issp, 10)
```

## Climate concern in 2020

:::: columns
::: {.column width="40%"}
There's a nice item in the ISSP where respondents could evaluate whether 

> "A rise in world's temperature is (dangerous/ not dangerous) for environment"^[https://access.gesis.org/dbk/77274]

:::

::: {.column width="60%"}
```{r}
#| echo: false
#| fig.asp: .8
likert_plot_2020
```
:::
::::

## EO indicators

We want to investigate whether temperature anomalies in the year of the survey, compared to a long-running average, are associated with climate change concerns.

1. *Indicator*: Temperature - annual average
2. *Intensity*: Anomaly (mean deviation)
3. *Focal time period*: 2020
4. *Baseline period*: 1961-1990
5. *Spatial buffer*: Country

## ERA5 data

The ERA5-Land Reanalysis from the Copernicus Climate Change Service is a suitable data product for this temperature indicator. It records observations on air temperature at 2 meters above the surface from 1950 onwards, has a spatial resolution of 0.1x0.1 degrees, and has global spatial coverage. We can apply our conceptualization exactly to these data.

![](../img/eo_indicators.png){.r-stretch fig-align="center"}

## Data access and preparation

To access the data, we need an ECMWF account. Utilizing the `ecmwfr` package, we can access the data directly in `R`. Given that we want to aggregate the data at the country level, we first load country vector data and download the data according to the spatial extent of the countries included in the survey. The ISSP has a diverse membership from North and South America, Europe, Africa, and Asia. Thus, we can work with a global spatial extent when downloading the EO indicator.

We also need some packages to load and prepare the world map and process the raster files (`rnaturalearth`, `sf`, `terra`, and `tidyverse`). We also need the `keyring` package to safely store our ECMWF-API key.

```{r}
#| eval: false
required_packages <- 
  c("keyring", "rnaturalearth", "sf", "tidyverse", "terra", "devtools")

new_packages <- 
  required_packages[!(required_packages %in% installed.packages()[,"Package"])]

if(length(new_packages)) install.packages(new_packages)

lapply(required_packages, library, character.only = TRUE)
```

## Country level shapefile

We load the vector data containing country-level polygons and subset it to the most relevant variables.

:::: columns
::: {.column width="50%"}
```{r}
#| eval: false
#| fig.asp: .8
world <- 
  rnaturalearth::ne_countries(
    scale = "medium", 
    returnclass = "sf"
  ) |> 
  dplyr::select(
    admin, 
    iso_a3, 
    geometry
  )

plot(sf::st_geometry(world))
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig.asp: .8
world <- 
  rnaturalearth::ne_countries(
    scale = "medium", 
    returnclass = "sf"
  ) |> 
  dplyr::select(
    admin, 
    iso_a3, 
    geometry
  )

plot(sf::st_geometry(world))
```
:::
::::

## Accessing the Copernicus API

A final step before we can access the data from the Copernicus API is to store our API key. The function automatically retrieves the key by setting it to `"wf_api_key"`.

```{r}
#| eval: false
# Store as environment variable
Sys.setenv(WF_API_KEY = "MY-API-KEY")

api_key <- Sys.getenv("WF_API_KEY")

keyring::key_set_with_value(service = "wf_api_key", password = api_key)
```

Now we can access the data. We loop the download over the four years of the survey program (1993, 2000, 2010, 2020) to create four separate files.

## Downloading the data

```{r}
#| eval: false
# API access looped over four years
for (yr in c("1993", "2000", "2010", "2020")) {
  
  # Create file names which include year
  file_name <- paste0("era5_temperature", yr, ".grib")
  
  # Specify API request
  request <- 
    list(
      data_format = "grib",
      variable = "2m_temperature",
      product_type = "monthly_averaged_reanalysis",
      time = "00:00",
      year = yr,
      month = 
        c(
          "01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"
        ),
      area = c(90, -180, -90, 180),
      dataset_short_name = "reanalysis-era5-land-monthly-means",
      target = file_name
    )
  
  # Download data from C3S
  file_path <- 
    ecmwfr::wf_request(
      request = request,
      transfer = TRUE,
      path = "./data/EO_data/C3S_data",
      verbose = FALSE
    )
}
```

## Importing the data

Importing the data into `R` should feel natural to us right now. We simply use `terra::rast()` for this endevaour.

```{r}
#| eval: false
temp_1993 <- terra::rast("./data/EO_data/C3S_data/era5_temperature1993.grib")
temp_2000 <- terra::rast("./data/EO_data/C3S_data/era5_temperature2000.grib")
temp_2010 <- terra::rast("./data/EO_data/C3S_data/era5_temperature2010.grib")
temp_2020 <- terra::rast("./data/EO_data/C3S_data/era5_temperature2020.grib")
```

```{r}
#| echo: false
temp_1993 <- terra::rast("../../data/EO_data/C3S_data/era5_temperature1993.grib")
temp_2000 <- terra::rast("../../data/EO_data/C3S_data/era5_temperature2000.grib")
temp_2010 <- terra::rast("../../data/EO_data/C3S_data/era5_temperature2010.grib")
temp_2020 <- terra::rast("../../data/EO_data/C3S_data/era5_temperature2020.grib")
```

## Inspecting the data

Let's inspect the datacube for 2020 and plot the first layer of the 2020 datacube (January 2020). The file's attributes tell us information on the dimensions (number of rows, columns, and layers), the resolution, spatial extent, the coordinate reference system, units, and time points.

:::: columns
::: {.column width="50%"}
```{r}
temp_2020
```
:::

::: {.column width="50%"}
```{r}
#| fig.asp: .7
plot(temp_2020[[1]])
```
:::
::::

## Aggregating to the country level

Now, we can aggregate the monthly values by year and country. If necessary, we will check that our country polygons and the raster files have the same CRS and align.

```{r}
for (yr in c("1993", "2000", "2010", "2020")) {
  temp_data <- get(paste0("temp_", yr))
  
  # Check CRS of both datasets and adjust if necessary
  if(!identical(terra::crs(world), terra::crs(temp_data))) {
    world <- 
      world |>
      sf::st_transform(sf::st_crs(temp_data))
  }
  
  # Collapse the month layers into one layer by averaging across months
  annual_values <- terra::app(temp_data, fun = mean, na.rm = TRUE, cores = 4)
  
  # Aggregate by country
  country_values <- 
    terra::extract(
      annual_values,
      world,
      fun = mean,
      na.rm = TRUE
    )
  
  # Add values to shapefile
  world[paste0("temp_", yr)] <- country_values[, 2]
}
```

## The result

We now have our country polygon vector file with yearly mean temperatures for each survey year.

```{r}
head(world, 2)
```

:::: columns
::: {.column width="50%"}
```{r}
#| eval: false
#| fig.asp: .4
plot(world["temp_2020"])
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig.asp: .4
plot(world["temp_2020"])
```
:::
::::

## Gathering the baseline data

Now that we have the focal values for all four survey years, we redo the process for the baseline period (1961-1990).

```{r}
#| eval: false
# Specify API request
request <- 
  list(
    data_format = "grib",
    variable = "2m_temperature",
    product_type = "monthly_averaged_reanalysis",
    time = "00:00",
    year = as.character(1961:1970),
    month = 
      c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"),
    area = c(90, -180, -90, 180),
    dataset_short_name = "reanalysis-era5-land-monthly-means",
    target = "era5_temperature1961-1990.grib"
  )

# Download data from C3S
file_path <- 
  ecmwfr::wf_request(
    request = request,
    transfer = TRUE,
    path = "./data/EO_data/C3S_data",
    verbose = FALSE
  )
```

## Importing the data

Again, we use `terra::rast()` to import the data.

```{r}
#| eval: false
temp_base <- 
  terra::rast("./data/EO_data/C3S_data/era5_temperature1961-1990.grib")
```

```{r}
#| echo: false
temp_base <- 
  terra::rast("../../data/EO_data/C3S_data/era5_temperature1961-1990.grib")
```


## Aggregating to the country level

We also aggregate these data at the country level and add them to our country polygon vector data.

```{r}
#| eval: false
# Check CRS of both datasets and adjust if necessary
if(!identical(terra::crs(world), terra::crs(temp_base))) {
  world <- 
    world |>
    sf::st_transform(crs = sf::st_crs(temp_base))
}

# Collapse all into one layer by averaging across months and years
annual_values <- terra::app(temp_base, fun = mean, na.rm = TRUE, cores = 4)

# Aggregate by country
country_values <- 
  terra::extract(
    annual_values,
    world,
    fun = mean,
    na.rm = TRUE
  )

# Add values to vector data
world$temp_base <- country_values[, 2]
```

```{r}
#| echo: false
# Check CRS of both datasets and adjust if necessary
if(!identical(terra::crs(world), terra::crs(temp_base))) {
  world <- 
    world |>
    sf::st_transform(crs = sf::st_crs(temp_base))
}

# Collapse all into one layer by averaging across months and years
annual_values_base <- 
  readRDS("../../data/EO_data/C3S_data/annual_values_base.rds")

# Aggregate by country
country_values <- 
  terra::extract(
    annual_values_base,
    world,
    fun = mean,
    na.rm = TRUE
  )

# Add values to vector data
world$temp_base <- country_values[, 2]
```


## The result

```{r}
head(world)
```

## Calculating deviations

Now that we have the focal and baseline values, we calculate single deviations.

:::: columns
::: {.column width="50%"}
```{r}
#| eval: false
#| fig.asp: .8
world <- 
  world |>
  dplyr::mutate(
    diff_1993 = temp_1993 - temp_base,
    diff_2000 = temp_2000 - temp_base,
    diff_2010 = temp_2010 - temp_base,
    diff_2020 = temp_2020 - temp_base
  )

# Plot 2020 deviation from baseline
ggplot(data = world) +
  geom_sf(aes(fill = diff_2020)) +
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(
    title = 
      "Absolute deviation between 2020 and baseline temperature",
    subtitle = "Averaged across countries",
    fill = "Temperature (K)"
  )
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig.asp: .8
world <- 
  world |>
  dplyr::mutate(
    diff_1993 = temp_1993 - temp_base,
    diff_2000 = temp_2000 - temp_base,
    diff_2010 = temp_2010 - temp_base,
    diff_2020 = temp_2020 - temp_base
  )

# Plot 2020 deviation from baseline
ggplot(data = world) +
  geom_sf(aes(fill = diff_2020)) +
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(
    title = 
      "Absolute deviation between 2020 and baseline temperature",
    subtitle = "Averaged across countries",
    fill = "Temperature (K)"
  )
```
:::
::::

## Let's use these data

Remember, our question was whether temperature deviations from a baseline period (1961-1990) in the survey year correspond with the overall climate concern within a country. We can assess this question, e.g., by aggregating the survey data as follows. Note that we create a within-country z-standardized measure for climate concern along the way.

:::: columns
::: {.column width="50%"}
```{r}
#| eval: false
issp_aggregated <-
  issp |> 
  dplyr::mutate(
    country = 
      ifelse(
        country == "USA", 
        "United States of America", 
        country
      )
  ) |> 
  dplyr::group_by(country) |> 
  dplyr::mutate(concern = scale(concern)) |> 
  dplyr::group_by(country, year) |> 
  dplyr::summarize(
    concern = mean(concern, na.rm = TRUE), 
    .groups = "drop"
  )

issp_aggregated
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
issp_aggregated <-
  issp |> 
  dplyr::mutate(
    country = 
      ifelse(
        country == "USA", 
        "United States of America", 
        country
      )
  ) |> 
  dplyr::group_by(country) |> 
  dplyr::mutate(concern = scale(concern)) |> 
  dplyr::group_by(country, year) |> 
  dplyr::summarize(
    concern = mean(concern, na.rm = TRUE), 
    .groups = "drop"
  )

issp_aggregated
```
:::
::::

## Structurally harmonzing the world data

Formally, the ISSP data are now in an aggregated long format. We have to wrangle our world data, including the temperature measures, to harmonize both datasets. Note that we z-standardize temperature differences across the whole period and countries this time, as they do not differ within countries in a specific year.

:::: columns
::: {.column width="50%"}
```{r}
#| eval: false
#| fig.asp: 1
world_restructured <-
  world |> 
  sf::st_drop_geometry() |> 
  dplyr::select(
    country = admin, 
    dplyr::contains("diff_")
  ) |> 
  tidyr::pivot_longer(
    cols = dplyr::contains("diff_"),
    values_to = "temp_diff",
    names_to = "diff"
  ) |> 
  dplyr::mutate(
    year = rep(c(1993, 2000, 2010, 2020), 242)
  ) |> 
  dplyr::mutate(temp_diff = scale(temp_diff))

world_restructured
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig.asp: 1
world_restructured <-
  world |> 
  sf::st_drop_geometry() |> 
  dplyr::select(
    country = admin, 
    dplyr::contains("diff_")
  ) |> 
  tidyr::pivot_longer(
    cols = dplyr::contains("diff_"),
    values_to = "temp_diff",
    names_to = "diff"
  ) |> 
  dplyr::mutate(
    year = rep(c(1993, 2000, 2010, 2020), 242)
  ) |> 
  dplyr::mutate(temp_diff = scale(temp_diff))

world_restructured
```
:::
::::

## Linking the data

We can now link both datasets to retrieve a final dataset we can analyze. We also converted it to a long format for plotting.

```{r}
issp_linked <-
  dplyr::left_join(
    issp_aggregated, world_restructured, by = c("country", "year")
  ) |> 
  tidyr::pivot_longer(
    cols = c("concern", "temp_diff"), names_to = "variable"
  )

issp_linked
```

## The analysis result

```{r}
#| echo: false
issp_linked |> 
  ggplot(aes(x = year, y = value, color = variable)) +
  geom_line() +
  facet_wrap(~country) +
  scale_x_continuous(breaks = issp_linked$year) +
  theme_bw()
```